\documentclass[11pt,a4paper]{article}
\usepackage{fullpage}
\input{z_marcro}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{microtype}
\usepackage{fouriernc}
\usepackage{graphicx}
\usepackage{times}
\usepackage{url}
\usepackage{listings}
\usepackage{color}
\usepackage{mathrsfs}
\usepackage{listings}
\usepackage{autobreak}
\usepackage{cite}
\definecolor{mygreen}{rgb}{0,0.4,0}
\definecolor{myblue}{rgb}{0,0,0.4}
\lstset{language=Matlab,
  basicstyle=\scriptsize\ttfamily,
  keywordstyle=\bfseries\color{black},
  commentstyle=\itshape\color{red},
  identifierstyle=\color{myblue},
  stringstyle=\color{mygreen},
  }
\title{Notes on Neural Networks based on primal-dual splitting method }
\author{Rui}

\begin{document}
\maketitle

\tableofcontents
\section{Possible paper production}
\begin{itemize}
	\item Training NN with ADMM (Prime-Dual); 
	\item Train CapsuleNet with ADMM
	\item Spectro-temporal Layer with ADMM
	\item CSC with ADMM
\end{itemize}
\section{Introduction}

\section{Problem formulation}

Let us consider a dataset $\mathcal{D} (\cu{X}, \cu{y})$. The loss functions for training an NN are usually MSE or crossentropy:

\begin{equation}
J(\cu{w}; \mathcal{D}) = L(\cu{w}; \mathcal{D}) + \lambda \phi (\cu{w})
\end{equation}

For MSE, the above equation generalizes to:

\begin{equation}
J(\cu{w}; \cu{X}, \cu{y}) = \frac{1}{N}\norm{\cu{y} - f(\cu{w}, \cu{X})}^2 + \lambda \phi(\cu{w})
\end{equation}

for crossentropy:

\begin{equation}
J(\cu{w}; \cu{X}, \cu{y}) = \sum^N_{i=1}(y_i\log f(\cu{w},\cu{X}) + (1-y_i)\log (1-f(\cu{w}, \cu{X}))) + \lambda \phi(\cu{w})
\end{equation}
where $\lambda \phi(\cu{w})$ can be any regularizers (L1, L2 etc.), and $f(\cdot)$ is network function. 

For a simplest NN example. Here we will use the notation: $\cu{x}\in \mathbb{R}^5: \cu{x} = \{x_1, x_2, ..., x_5\}$ and $y: y\in \mathbb{R}$ as the pair of data for training; $w^l_{ij}$ denotes the weight that connects $i$-th neuron on $(l-1)$-th layer and $j$-th neuron on $l$-th layer; $\sigma_l(\cdot)$ is the activation function for $l$-th layer; $o^l_i$ is the output from the $i$-th neuron of $l$-th layer; $Out$ is the output of the neural network (last neuron): 

\begin{figure}[h!]
	\includegraphics[scale=0.75]{figs/nn}
	\centering
\end{figure}

The loss is:

\begin{equation}
J(\cu{W}; \cu{x}, y) = \frac{1}{2} (Out_{\cu{W},\cu{x}} - y)^2 + \text{regularizer on } \cu{w}
\end{equation}

Let us show some forward example of it. Take output $Out$:
\begin{align}
Out &= \sigma_3(o^2_1w^3_{11} + o^2_2w^3_{21} + b^3)\\
&= \sigma_3(\cu{o}^2\cu{w}^3 + b_3)\\
&= \sigma_3(\Sigma^3_1)
\end{align}
where $\cu{o}^2 = [o^2_1, o^2_2]$, $\cu{w}^3 = [w^3_{11}, w^3_{21}]^T$ and $\Sigma^3_1 = \cu{o}^2\cu{w}^3 + b_3$.
Let us continue for $o^2_1$ and $o^2_2$:
\begin{align}
o^2_1 &= \sigma_2(o^1_1w^2_{11} + o^1_2w^2_{21} + o^1_3w^2_{31} + b^2_1)\\
&= \sigma_2(\Sigma^2_1)\\
o^2_2 &= \sigma_2(o^1_1w^2_{12} + o^1_2w^2_{22} + o^1_3w^2_{32} + b^2_2)\\
&= \sigma_2(\Sigma^2_2)\\
\cu{o}^2 &= \sigma_2(\cu{o}^1\cu{w}^2 + \cu{b}^2)
\end{align}
where $\cu{o}^1 = [o^1_1, o^1_2, o^1_3]$ and $\cu{w}^2 = \begin{bmatrix}
w^2_{11} & w^2_{21} & w^2_{31}\\
w^2_{12} & w^2_{22} & w^2_{32}
\end{bmatrix}^T$. and $\cu{o}^1 = \sigma_1(\cu{x}\cu{w}^1 + \cu{b}^1)$. 

In traditional approach, we need to derive the gradient of the loss w.r.t each weight i.e.:

\begin{equation}
\tash{L(\cu{W}; \cu{x}, y)}{w^l_{ij}}
\end{equation}

Those gradient can be efficiently obtained by backpropogation (BP). For simplicity, BP not discussed here. 

If we use SGD, the update of weight is just: $w^l_{ij}(k+1) = w^l_{ij}(k) + \eta \tash{L(\cu{W}; \cu{x}, y)}{w^l_{ij}}$.

So what about updating the weight using ADMM or Prime-Dual? The only motivation for doing this is that we can use fancier regularizers (e.g. $\norm{\bm{\Omega} \cu{w}}$)?

\section{ADMM solver}
We consider the original optimization problem~\eqref{obj_opt}. Firstly, we introduce an auxiliary variable $v$, and have the following constrained problem  
\begin{equation}\label{eq:admm}
\begin{split}
\min_{x} &\frac{1}{2} \| y - H \, x \|^2_{R^{-1}}
   + \frac{1}{2} \| \Psi \, x - m \|^2_{Q^{-1}} 
   + \lambda \|\Omega\, x \|_1  \\
\mathrm{s.t.}\ & v = \Omega\,x . 
\end{split}
\end{equation}
Then, Let $\mathcal{L_\rho}(x,v;\eta)$ be the augmented Lagrangian function of~Eq.~\eqref{eq:admm} which is defined as follows
\begin{equation}\label{Lagrangian_func}
\begin{split}
\begin{aligned}
\mathcal{L_\rho}(x,v;\eta) \triangleq  \frac{1}{2}  \left \|  H\,x-y \right \|_{R^{-1}}^2 + \lambda\left \| v \right \|_1 
+ \frac{1}{2} \left \| \Psi \, x - m\right\|_{ Q^{-1} }^2    
+ \frac{\rho}{2}\left \| v- \Omega \,x + \eta \right \|^2
\end{aligned}
\end{split}
\end{equation}
The solution of~\eqref{Lagrangian_func} can be obtained by the following steps \cite{Boyd2011ADMM}:
\begin{enumerate}
\item Input: $ v^0$, $\eta^0$, $\rho$,
\item Loop: For $k=1,2,\cdots$ until termination criteria are met.
\begin{itemize}
\item Calculate $x^k $ by solving: 
\begin{equation}
\label{x_update}
\begin{split}
x^{k+1} &= \arg \min_x \  \mathcal{L_\rho}(x,v^k;\eta^k) \\
        &= \arg \min_x \   \underbrace{\frac{1}{2} \left\|  H x- y \right\|_{R^{-1}}^2 
          + \frac{1}{2} \left\| \Psi x - m \right\|_{Q^{-1} }^2
          + \frac{\rho}{2} \left\| v- \Omega\,x + \eta \right\|^2 }_{E(x)}
\end{split}
\end{equation}
We then compute the gradient of $E(x)$ by
%
\begin{equation}
\begin{split}
   \nabla E(x)
   &= -H^\top R^{-1} ( y - H \, x )
   +  \Psi^\top Q^{-1} ( \Psi \, x - m ) 
   - \rho \, \Omega^\top ( v - \Omega \, x + \eta ) \\
   &= H^\top R^{-1} H \, x - H^\top R^{-1} y
   +  \Psi^\top Q^{-1}  \Psi \, x  - \Psi^\top Q^{-1} m  \\
   &\qquad + \rho \, \Omega^\top \Omega \, x - \rho \, \Omega^\top ( v + \eta )  \\
   &= \left[ H^\top R^{-1} H 
   +  \Psi^\top Q^{-1}  \Psi 
   + \rho \, \Omega^\top \Omega \right] \, x  \\
   & \qquad - \left[ H^\top R^{-1} y + \Psi^\top Q^{-1} m + \rho \, \Omega^\top ( v + \eta )  \right]
   = 0
\end{split}
\end{equation}
%
Thus, we could have
%
\begin{equation}
\begin{split}
  x^{k+1} &= \left[ H^\top R^{-1} H 
   +  \Psi^\top Q^{-1}  \Psi 
   + \rho \, \Omega^\top \Omega \right]^{-1} \\
   & \qquad \times \left[ H^\top R^{-1} y + \Psi^\top Q^{-1} m + \rho \, \Omega^\top ( v^k + \eta^k )  \right]
\end{split}
\end{equation}

or, we could use RTS smoother to solve the problem~\eqref{x_update} 

\item Calculate $v^k $ by solving: 
\begin{equation}
\begin{split}
v^{k+1} &= \arg \min_v \ \mathcal{L_\rho}(x^{k+1},v;\eta^k)  \\
&= \lambda \left \| v \right \|_1+\frac{\rho}{2}\left \| v- \Omega x^{k+1} + \eta^{k}\right \|^2 \\
&= \operatorname{sign}(\Omega x^{k+1} + \eta^k/\rho) \circ \operatorname{max} (|\Omega x^{k+1} + \eta^k/\rho|-{\lambda}/{\rho},0 )
\end{split}
\end{equation}
where $\operatorname{sign}$ represents the signum function, and $\circ$ is the pointwise product. 
\item Calculate $\eta^k $ by solving:
\begin{equation}
\begin{split} 
\eta^{k+1}= \eta^{k}+ \rho (\Omega\,x^{k+1} -v^{k+1} )
\end{split}
\end{equation}
\end{itemize}
\item Output: $x^{k+1}$
\end{enumerate}



\section{A first-order Primal-dual splitting algorithm}


\section{Matlab example}
%\lstinputlisting{example/***.m} 

\bibliography{note_refs}{}
\bibliographystyle{plain}
\end{document}
